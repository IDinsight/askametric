{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions before running this notebook\n",
    "---\n",
    "\n",
    "1. Follow the instructions in the `README` at the root of this repository to set up the required packages for Ask-a-Metric.\n",
    "\n",
    "2. Create an account on vanna.ai and get an API key (skip if you already have a [vanna.ai](https://vanna.ai) account)\n",
    "\n",
    "3. On your vanna.ai account, create **2 RAG models**. If you are running this notebook with existing models, please reset training data, in order to ensure you are working with \"fresh\" models.\n",
    "\n",
    "4. Locally in your AAM conda environment / poetry shell, run `pip install vanna pandas`\n",
    "\n",
    "5. Inside the `experiments/` folder, create a `.env` file with the follwing env variables:\n",
    "    ```\n",
    "    VANNA_API_KEY=<your vanna.ai API key, downloaded in step 2>\n",
    "\n",
    "    VANNA_RAG_MODEL_WITH_SCHEMA=<name of first RAG model you created in step 3>\n",
    "    \n",
    "    VANNA_RAG_MODEL_WITHOUT_SCHEMA='<name of second RAG model you created in step 3>\n",
    "    \n",
    "    OPENAI_API_KEY=<your OpenAI API key>\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "from askametric.query_processor.query_processor import LLMQueryProcessor\n",
    "from askametric.utils import _ask_llm_json\n",
    "\n",
    "from vanna.openai import OpenAI_Chat\n",
    "from vanna.vannadb import VannaDB_VectorStore\n",
    "\n",
    "from litellm import cost_per_token, token_counter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.asyncio import (\n",
    "    AsyncSession,\n",
    "    create_async_engine,\n",
    ")\n",
    "\n",
    "# Load Environment Variables\n",
    "load_dotenv(\".env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Parameters\n",
    "OPEN_AI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "WHICH_DB = \"tn_covid_cases_11_may\"\n",
    "LLM_MODEL = \"gpt-4o\"\n",
    "\n",
    "# AAM-specific Parameters\n",
    "GUARDRAILS_LLM_MODEL = \"gpt-4o\"\n",
    "SYSTEM_MESSAGE = \"Government and health officials in Tamil Nadu, India will ask you questions.\\\n",
    "      You need to help them manage active COVID cases and the availablity of beds in health facilities.\"\n",
    "DB_TABLE_DESCRIPTION = \"- bed_vacancies_clinics_may_11: Each row identifies a district and the beds earmarked, occupied and available for COVID cases in the district clinics.\\\n",
    "- bed_vacancies_health_centers_and_district_hospitals_11_may: Each row identifies a district and the beds earmarked, occupied and available, with and without oxygen supply, and with and without ICU support, for COVID cases in the disctrict health centers and hospitals.\\\n",
    "- covid_cases_11_may: Each row identifies a district and the number of people who received treatment, were discharged and died due to COVID.\\\n",
    "\"\n",
    "DB_COLUMN_DESCRIPTION = \"\"\n",
    "NUM_COMMON_VALUES = 10\n",
    "DB_PATH = \"../demo_databases/tn_covid_cases_11_may.sqlite\"\n",
    "\n",
    "\n",
    "# Vanna-specific Parameters\n",
    "VANNA_AI_API_KEY = os.getenv(\"VANNA_API_KEY\")\n",
    "VANNA_RAG_MODEL_WITH_SCHEMA = os.getenv('VANNA_RAG_MODEL_WITH_SCHEMA')\n",
    "VANNA_RAG_MODEL_WITHOUT_SCHEMA = os.getenv('VANNA_RAG_MODEL_WITHOUT_SCHEMA')\n",
    "# these parameters are set to imitate the AAM setup as closely as possible,\n",
    "# to allow a fair comparison between the two systems\n",
    "ALLOW_AUTO_TRAIN_ON_QUERIES = False\n",
    "VISUALIZE = False\n",
    "ALLOW_LLM_TO_SEE_DATA_DURING_RAG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up evaluation prompts for text responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_eval_message_template = \"\"\"\n",
    "----Message Begins----------------\n",
    "Question: {question}\n",
    "Answer: {text_response}\n",
    "Correct Answer: {correct_answer}\n",
    "Correct Language: {correct_language}\n",
    "----Message Ends----------------\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_MESSAGE_TEXT_EVAL = \"\"\"\n",
    "You are a grading bot. You will get messages in the following format -\n",
    "\n",
    "----Message Begins----------------\n",
    "Question: ```<Some Question>```\n",
    "Answer: ```<Answer to be graded>```\n",
    "Correct Answer: ```<Correct Answer to Question>```\n",
    "Correct Language: ```<Correct Language the \"Answer\" should be in with the language script if relevant>```\n",
    "----Message Ends----------------\n",
    "\n",
    "Give grades based on the following two points:\n",
    "(a) Accuracy and Relevancy:\n",
    "Is the \"Answer\" similar in meaning to \"Correct Answer\"?\n",
    "Does the \"Answer\" address the key elements of the \"Question\"?\n",
    "If yes, give a grade of 1, otherwise 0.\n",
    "Remember, \"Answer\" and \"Correct Answer\" ONLY NEED TO BE SIMILAR in general meaning.\n",
    "Look at the NUMBERS in the response to make you judgement.\n",
    "(b) Language and Script:\n",
    "Is \"Answer\" in the \"Correct Language\"? Is \"Answer\" in the same script as \"Question\"?\n",
    "If yes, give a grade of 1, otherwise 0.\n",
    "\n",
    "REMEMBER: Scores in each category (a or b) can ONLY be 0 or 1\n",
    "\n",
    "Reply in the following json format -\n",
    "{\"accuracy_and_relevancy\": <your grade for accuracy and relevancy>,\n",
    "\"language_and_script\": <your grade for language and script>}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up utility functions for Vanna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions for Vanna\n",
    "def setup_vanna_ai(\n",
    "        vanna_rag_model,\n",
    "        train_with_sql_schema: bool = False,\n",
    "        system_message: str = SYSTEM_MESSAGE,\n",
    "        db_path: str = DB_PATH):\n",
    "    \"\"\"\n",
    "    Setup Vanna.ai Model\n",
    "\n",
    "    Args:\n",
    "        vanna_rag_model: the RAG model to use for Vanna\n",
    "        train_with_sql_schema: whether to train the model with the SQL schema\n",
    "        system_message: the system message for Vanna.ai\n",
    "        db_path: the path to the database\n",
    "    \"\"\"\n",
    "    class MyVanna(VannaDB_VectorStore, OpenAI_Chat):\n",
    "        def __init__(self, config=None):\n",
    "            VannaDB_VectorStore.__init__(\n",
    "                self,\n",
    "                vanna_model=vanna_rag_model,\n",
    "                vanna_api_key=VANNA_AI_API_KEY,\n",
    "                config=config)\n",
    "            OpenAI_Chat.__init__(self, config=config)\n",
    "\n",
    "    vn = MyVanna(\n",
    "        config={'api_key': OPEN_AI_API_KEY, 'model': LLM_MODEL}\n",
    "    )\n",
    "\n",
    "    vn.connect_to_sqlite(db_path)\n",
    "    vn.train(documentation = system_message + \"\\n\\n\" +\n",
    "             DB_TABLE_DESCRIPTION + \"\\n\\n\" + DB_COLUMN_DESCRIPTION)\n",
    "\n",
    "    if train_with_sql_schema:\n",
    "        # Train vanna on SQL schema\n",
    "        df_ddl = vn.run_sql(\"SELECT type, sql FROM sqlite_master WHERE sql is not null\")\n",
    "        for ddl in df_ddl['sql'].to_list():\n",
    "            vn.train(ddl=ddl)\n",
    "\n",
    "    return vn\n",
    "\n",
    "\n",
    "async def get_vanna_reponse(vn, question: pd.DataFrame | str):\n",
    "    \"\"\"\n",
    "    Send request to the vanna.ai and extract responses for a single question\n",
    "\n",
    "    Args:\n",
    "        vn: the Vanna model\n",
    "        question: the question to ask\n",
    "\n",
    "    Returns: a dictionary of vanna's response and associated costs\n",
    "    \"\"\"\n",
    "    # Get response\n",
    "    tic = time.time()  # record the start time\n",
    "    vn_answer = vn.ask(\n",
    "        question if isinstance(question, str) else question.question,\n",
    "        print_results=False,\n",
    "        auto_train=ALLOW_AUTO_TRAIN_ON_QUERIES,\n",
    "        visualize=VISUALIZE,\n",
    "        allow_llm_to_see_data=ALLOW_LLM_TO_SEE_DATA_DURING_RAG\n",
    "        )\n",
    "    response_time = time.time() - tic  # calculate the response time\n",
    "\n",
    "    prompt_token_count = token_counter(\n",
    "        text = vn.get_sql_prompt(\n",
    "            initial_prompt=\"\",\n",
    "            question=question if isinstance(question, str) else question.question,\n",
    "            question_sql_list=[],\n",
    "            ddl_list=[],\n",
    "            doc_list=[DB_TABLE_DESCRIPTION]\n",
    "        )\n",
    "    )\n",
    "    response_token_count = token_counter(text=str(vn_answer))\n",
    "\n",
    "    cost = cost_per_token(\n",
    "        model=LLM_MODEL,\n",
    "        prompt_tokens=prompt_token_count,\n",
    "        completion_tokens=response_token_count,\n",
    "    )\n",
    "\n",
    "    # Vanna returns None if SQL query couldn't be processed\n",
    "    if not vn_answer:\n",
    "        text_response = \"\"\n",
    "        sql_response = \"\"\n",
    "    else:\n",
    "        text_response = str(vn_answer[1])\n",
    "        sql_response = str(vn_answer[0])\n",
    "\n",
    "    if not isinstance(question, str):\n",
    "        evaluation_json = await _ask_llm_json(\n",
    "                prompt=text_eval_message_template.format(\n",
    "                    question=question.question,\n",
    "                    text_response=text_response,\n",
    "                    correct_answer=question.correct_answer,\n",
    "                    correct_language=question.language\n",
    "                    ),\n",
    "                system_message=SYSTEM_MESSAGE_TEXT_EVAL,\n",
    "                llm=LLM_MODEL)\n",
    "        evaluation_json = evaluation_json['answer']\n",
    "    else:\n",
    "        evaluation_json = {'accuracy_and_relevancy': 'N/A',\n",
    "                           'language_and_script': 'N/A'}\n",
    "\n",
    "\n",
    "    # Save only the sql query and text outputs along with the cost,\n",
    "    response = {\n",
    "        \"question\": question if isinstance(question, str) else question.question,\n",
    "        \"vanna_cost\": sum(cost),\n",
    "        \"vanna_response_time\": response_time,\n",
    "        \"vanna_text_response\": text_response,\n",
    "        \"vanna_sql_query\": sql_response,\n",
    "        \"vanna_accuracy_and_relevancy\": evaluation_json['accuracy_and_relevancy'],\n",
    "        \"vanna_language_and_script\": evaluation_json['language_and_script']\n",
    "    }\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up utility functions for AAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_asession(db_path: str = DB_PATH):\n",
    "    \"\"\"\n",
    "    Get assession for db schema and aam\n",
    "\n",
    "    Args:\n",
    "        db_path: the path to the database\n",
    "    \"\"\"\n",
    "    aengine = create_async_engine(\n",
    "        url=f\"sqlite+aiosqlite:///{db_path}\"\n",
    "    )\n",
    "\n",
    "    async_session = sessionmaker(\n",
    "        bind=aengine,\n",
    "        class_=AsyncSession,\n",
    "        expire_on_commit=False\n",
    "    )\n",
    "\n",
    "    return async_session\n",
    "\n",
    "async def get_aam_reponse(question: pd.DataFrame | str,\n",
    "                          asession: AsyncSession,\n",
    "                          system_message: str = SYSTEM_MESSAGE,\n",
    "                          db_description: str = DB_TABLE_DESCRIPTION,\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Send single query to the LLM\n",
    "\n",
    "    Args:\n",
    "        question: a single validation question\n",
    "        asession: the async session for the database\n",
    "        system_message: the system message for AAM\n",
    "        db_description: the description of the database\n",
    "\n",
    "    Returns: a dictionary of aam's response and associated costs\n",
    "    \"\"\"\n",
    "    tic = time.time()  # record the start time\n",
    "\n",
    "    async with asession() as session:\n",
    "        qp = LLMQueryProcessor(\n",
    "            query={\"query_text\": question if isinstance(question, str) else question.question,\n",
    "                   \"query_metadata\": {}},\n",
    "            asession=session,\n",
    "            which_db=WHICH_DB,\n",
    "            llm=LLM_MODEL,\n",
    "            guardrails_llm=GUARDRAILS_LLM_MODEL,\n",
    "            sys_message=system_message,\n",
    "            db_description=db_description,\n",
    "            column_description=DB_COLUMN_DESCRIPTION,\n",
    "            num_common_values=NUM_COMMON_VALUES\n",
    "        )\n",
    "        try:\n",
    "            await qp.process_query()\n",
    "        except Exception as e:\n",
    "            qp.final_answer = str(e)\n",
    "            qp.sql_query = \"\"\n",
    "\n",
    "        toc = time.time()  # record the end time\n",
    "        response_time = toc - tic  # calculate the response time\n",
    "\n",
    "        if not isinstance(question, str):\n",
    "            evaluation_json = await _ask_llm_json(\n",
    "                prompt=text_eval_message_template.format(\n",
    "                    question=question.question,\n",
    "                    text_response=qp.final_answer,\n",
    "                    correct_answer=question.correct_answer,\n",
    "                    correct_language=question.language\n",
    "                    ),\n",
    "                system_message=SYSTEM_MESSAGE_TEXT_EVAL,\n",
    "                llm=LLM_MODEL)\n",
    "            evaluation_json = evaluation_json['answer']\n",
    "        else:\n",
    "            evaluation_json = {'accuracy_and_relevancy': 'N/A',\n",
    "                               'language_and_script': 'N/A'}\n",
    "\n",
    "\n",
    "\n",
    "        # Save only the sql query and text outputs along with the cost\n",
    "        response = {\n",
    "            \"question\": question if isinstance(question, str) else question.question,\n",
    "            \"aam_cost\": qp.cost,\n",
    "            \"aam_response_time\": response_time,\n",
    "            \"aam_text_response\": qp.final_answer,\n",
    "            \"aam_sql_query\": qp.sql_query,\n",
    "            \"aam_accuracy_and_relevancy\": evaluation_json[\"accuracy_and_relevancy\"],\n",
    "            \"aam_language_and_script\": evaluation_json[\"language_and_script\"]\n",
    "        }\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vanna.ai and AAM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Run this block ONCE -----\n",
    "# NB: we train 2 instances, with and without the SQL schema for subsequent experiments\n",
    "# NNB: we created FRESH models on each run of this notebook, so we don't need to worry about\n",
    "# overwriting the models with the same name, or working with pre-trained models\n",
    "vn_without_schema = setup_vanna_ai(\n",
    "    VANNA_RAG_MODEL_WITHOUT_SCHEMA,\n",
    "    train_with_sql_schema=False)\n",
    "\n",
    "vn_with_schema = setup_vanna_ai(\n",
    "    VANNA_RAG_MODEL_WITH_SCHEMA,\n",
    "    train_with_sql_schema=True)\n",
    "\n",
    "# Prepare the db for AAM\n",
    "async_session = get_db_asession()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation questions\n",
    "eval_questions = pd.read_csv(\n",
    "            \"tn_covid_val_questions.csv\",\n",
    "            skip_blank_lines=True\n",
    "        ).dropna(how=\"all\")\n",
    "eval_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run through basic, language and guardrail questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get responses from AAM\n",
    "tasks = [get_aam_reponse(eval_row, async_session) for _, eval_row in eval_questions.iterrows()]\n",
    "aam_responses = await asyncio.gather(*tasks)\n",
    "aam_responses = pd.DataFrame(aam_responses)\n",
    "aam_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get responses from Vanna model without schema\n",
    "tasks = [get_vanna_reponse(vn_without_schema, eval_row)\n",
    "         for _, eval_row in eval_questions.iterrows()]\n",
    "vn_without_schema_responses = await asyncio.gather(*tasks)\n",
    "vn_without_schema_responses = pd.DataFrame(vn_without_schema_responses)\n",
    "\n",
    "vn_without_schema_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get responses from Vanna model with schema\n",
    "tasks = [get_vanna_reponse(vn_with_schema, eval_row)\n",
    "         for _, eval_row in eval_questions.iterrows()]\n",
    "vn_with_schema_responses = await asyncio.gather(*tasks)\n",
    "vn_with_schema_responses = pd.DataFrame(vn_without_schema_responses)\n",
    "\n",
    "vn_with_schema_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine responses and save\n",
    "vanna_merge = pd.merge(\n",
    "    left=vn_without_schema_responses,\n",
    "    right=vn_with_schema_responses,\n",
    "    on=\"question\",\n",
    "    suffixes=(\"_without_schema\", \"_with_schema\")\n",
    ")\n",
    "all_response = vanna_merge.merge(aam_responses, on=\"question\")\n",
    "all_response_plus_eval = all_response.merge(eval_questions, on=\"question\")\n",
    "all_response_plus_eval.to_csv(\"tn_covid_all_responses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare accuracy, relevancy and language of responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy and Relevancy Scores\")\n",
    "print(all_response.filter(like=\"accuracy_and_relevancy\").mean(0))\n",
    "\n",
    "print(\"Language and Script Scores\")\n",
    "print(all_response.filter(like=\"language_and_script\").mean(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare cost and response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average cost per query (in USD)\n",
    "\n",
    "# NB -- this might NOT be a fair comparison, since we only compute\n",
    "# input and output token costs for vanna.ai (and not training or for intermediate RAG queries)\n",
    "# Costs for AAM, however, are computed for every OpenAI API call in the pipeline.\n",
    "\n",
    "all_response.filter(like=\"cost\").mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average response time per query (in s)\n",
    "\n",
    "all_response.filter(like=\"response_time\").mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare how easy it is to make quick updates to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to update the system prompt, to ask the LLM so that\n",
    "# it defaults to providing answers about Chennai, when no location is specified\n",
    "# in the query. This is useful on AAM, since we often encounter user queries\n",
    "# that could be ambiguous\n",
    "\n",
    "# Update the system message\n",
    "updated_system_message = SYSTEM_MESSAGE + \"\\n\" + \\\n",
    "    \"REMEMBER: If the user query does not specify a district, assume it is about Chennai.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain vanna.ai with the updated system message\n",
    "\n",
    "# Remove previous training data for vanna.ai without schema\n",
    "[vn_without_schema.remove_training_data(id=id)\n",
    " for id in vn_without_schema.get_training_data().id.values]\n",
    "\n",
    "# Remove previous training data for vanna.ai with schema\n",
    "[vn_with_schema.remove_training_data(id=id)\n",
    " for id in vn_with_schema.get_training_data().id.values]\n",
    "\n",
    "# Retrain vanna.ai without schema\n",
    "vn_without_schema = setup_vanna_ai(\n",
    "    VANNA_RAG_MODEL_WITHOUT_SCHEMA,\n",
    "    train_with_sql_schema=False,\n",
    "    system_message=updated_system_message\n",
    ")\n",
    "\n",
    "# Retrain vanna.ai with schema\n",
    "vn_with_schema = setup_vanna_ai(\n",
    "    VANNA_RAG_MODEL_WITH_SCHEMA,\n",
    "    train_with_sql_schema=True,\n",
    "    system_message=updated_system_message\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response from AAM, Vanna without schema, and Vanna with schema\n",
    "# with ambiguous queries\n",
    "\n",
    "question = \"How many ICU beds?\"\n",
    "\n",
    "# AAM\n",
    "aam_response = await get_aam_reponse(question, async_session, updated_system_message)\n",
    "\n",
    "# Vanna without schema\n",
    "vn_without_schema_response = await get_vanna_reponse(vn_without_schema, question)\n",
    "\n",
    "# Vanna with schema\n",
    "vn_with_schema_response = await get_vanna_reponse(vn_with_schema, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AAM Response: {aam_response['aam_text_response']}\")\n",
    "print(f\"Vanna without schema Response: {vn_without_schema_response['vanna_text_response']}\")\n",
    "print(f\"Vanna with schema Response: {vn_with_schema_response['vanna_text_response']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "askametric-public",
   "language": "python",
   "name": "askametric-public"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
